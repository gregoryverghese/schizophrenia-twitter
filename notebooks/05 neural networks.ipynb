{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains neural network code. Here we build a 1D-CNN and a LSTM architecture and use it to classify each of our embedding models. This follows much the same logic as '04 machine learning' notebooks. I use Keras with a Tensorflow backend. I use code from the utility, feature_engineering, preprocessing and embeddings scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "import utility\n",
    "import feature_engineering as fe\n",
    "import preprocessing as pp\n",
    "import embeddings as em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import functools\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D\n",
    "from keras.layers import Dropout, Activation, GlobalMaxPooling1D, GlobalAveragePooling1D, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up all the neccessary parameters as well as the parameter lists to be used in the random search parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 1\n",
    "cbowTrain = 0\n",
    "sgTrain = 1\n",
    "wordToken = False\n",
    "charToken = True\n",
    "m1EClean=['Tokens', 'Lemma', 'Stopwords', 'Phrases', 'Emoticons']\n",
    "m2EClean=['Tokens', 'Lemma', 'Stopwords', 'Phrases']\n",
    "m1LClean=['Tokens', 'Lemma', 'Stopwords', 'Phrases', 'Lowercase']\n",
    "m2LClean=['Tokens', 'Lemma', 'Stopwords', 'Phrases']\n",
    "gloveInFileName = 'data/glove/glove.twitter.27B.'\n",
    "gloveOutFileName = 'gensim_glove_vectors_'\n",
    "gloveDim = ['25d.txt', '50d.txt', '100d.txt', '200d.txt']\n",
    "fileNameSchiz1 = 'data/dataOut/schiz/annFinalSchiz_1.csv'\n",
    "fileNameSchiz2 = 'data/dataOut/schiz/annFinalSchiz_2.csv'\n",
    "fileNameStig = 'data/dataOut/stigma/annFinalStig.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "socialDf = pd.read_csv(fileNameSchiz1, encoding='utf-8')\n",
    "textSchiz1 = socialDf['Tweet']\n",
    "labelsSchiz1 = socialDf['Classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "socialDf = pd.read_csv(fileNameSchiz2, encoding='utf-8')\n",
    "textSchiz2 = socialDf['Tweet']\n",
    "labelsSchiz2 = socialDf['Classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "socialDf = pd.read_csv(fileNameStig, encoding='utf-8')\n",
    "textStigma = socialDf['Tweet']\n",
    "labelsStigma = socialDf['Classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set up the various parameters to be optimized over\n",
    "'''\n",
    "\n",
    "dropout = [0.0, 0.1, 0.2, 0.3, 0.4, 0.6]\n",
    "neurons = [5, 10, 20, 50, 100]\n",
    "activation = ['softmax', 'relu', 'tanh', 'sigmoid']\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adam']\n",
    "\n",
    "cnnParamGrid = dict(numFilters=[32, 64, 128],\n",
    "                  kernelSize=[3, 5, 10],\n",
    "                 dropout=dropout,\n",
    "                 neurons=neurons,\n",
    "                 activation1=['relu', 'sigmoid', 'softmax'],\n",
    "                 activation2=['relu', 'sigmoid', 'softmax'],\n",
    "                 optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The code in this cell is taken from\n",
    "https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "it calculates metrics for the neural networks\n",
    "'''\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "     auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "     K.get_session().run(tf.local_variables_initializer())\n",
    "     return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define three models, the lSTM bidirectional model was not used in the paper, just the lSTM and CNN. These models are then used to fit and train on our embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLSTMBidrectional(embeddingsMatrix, vocabDim, embeddingDim, sequenceDim, activation='sigmoid', train=False, dropout=0.3, \n",
    "            reDropout=0.3, optimizer='RMSprop', neurons=100, metrics=['acc',f1_m,precision_m, recall_m]):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabDim, embeddingDim, weights=[embeddingsMatrix], input_length=sequenceDim, trainable=train))\n",
    "    model.add(Bidirectional(LSTM(neurons, dropout=dropout, recurrent_dropout=reDropout, return_sequences=False)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "'''\n",
    "LSTM model from here\n",
    "code has been adapted from here\n",
    "https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "'''\n",
    "def getLSTMModel(embeddingsMatrix, vocabDim, embeddingDim, sequenceDim, activation='sigmoid', train=False, dropout=0.3, \n",
    "            reDropout=0.3, optimizer='RMSprop', neurons=100, metrics=['acc',f1_m,precision_m, recall_m]):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabDim, embeddingDim, weights=[embeddingsMatrix], input_length=sequenceDim, trainable=train))\n",
    "    model.add(LSTM(neurons, dropout=dropout, recurrent_dropout=reDropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "1DCNN models with one CNN layer - code has been adapted from \n",
    "https://realpython.com/python-keras-text-classification/#convolutional-neural-networks-cnn\n",
    "'''\n",
    "def getCNNModel(embeddingsMatrix, vocabDim, embeddingDim,  sequenceDim, kernelSize=3, \n",
    "                numFilters=64, activation1='relu', activation2='sigmoid', dropout=0.1, \n",
    "                neurons=20, activation3=None, optimizer='SGD', train=False, metrics=['acc',f1_m,precision_m, recall_m]):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabDim, embeddingDim, weights=[embeddingsMatrix], input_length=sequenceDim, trainable=train))\n",
    "    model.add(Conv1D(numFilters, kernelSize, activation=activation1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(neurons, activation=activation1))\n",
    "    model.add(Dense(1, activation=activation2))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "'''\n",
    "performs the random parameter tuning uses keras classifier to wrap around the scikkit-learn tuning module\n",
    "'''\n",
    "def getRandom(nn, paramGrid, xTrain, yTrain, xTest, yTest, epochs=50, batch=10, cv=5, niter=50):\n",
    "\n",
    "    model = KerasClassifier(build_fn=nn, epochs=epochs, batch_size=batch, verbose=False)\n",
    "    random = RandomizedSearchCV(estimator=model, param_distributions=paramGrid, cv=cv, verbose=1, n_iter=niter)\n",
    "    \n",
    "    random = random.fit(xTrain, yTrain)\n",
    "    testAccuracy = random.score(xTest, yTest)\n",
    "\n",
    "    return (random, testAccuracy)\n",
    "    \n",
    "'''\n",
    "fit the model to train data\n",
    "'''   \n",
    "def fitModel(model, xTrain, yTrain, xTest, yTest, epochs=100):\n",
    "    \n",
    "    model.fit(xTrain, yTrain, epochs=epochs, validation_split=0.1)\n",
    "    loss, accuracy, f1_score, precision, recall = model.evaluate(xTest, yTest, verbose=False)\n",
    "    \n",
    "    return (loss, accuracy, f1_score, precision, recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "initiialize preprocessing\n",
    "'''\n",
    "\n",
    "def initializePreProcessing(text, tokenType, cleanMethods=['Tokens', 'Lemma', 'Stopwords', 'Phrases', 'Emoticons']):\n",
    "\n",
    "    social = pp.SocialPreProcessing(text, tokenType)\n",
    "    socialClean = social.clean(cleanMethods)\n",
    "    \n",
    "    return socialClean\n",
    "\n",
    "'''\n",
    "get features and pad each tweet to a length of 200. Anything longer gets truncated\n",
    "'''\n",
    "def getFeatures(embed, wIndex, tokens, labels):\n",
    "    \n",
    "    indexData = embed.getIndexData(tokens, labels, wIndex)\n",
    "    \n",
    "    features = indexData[0]\n",
    "    labels = indexData[1]\n",
    "    \n",
    "    features = pad_sequences(features, maxlen=200, dtype='int32', padding='pre', truncating='pre', value=0.0)\n",
    "    \n",
    "    return (features, labels)\n",
    "\n",
    "'''\n",
    "initialize the tuning of parameters\n",
    "'''\n",
    "def getTune(**kwargs):\n",
    "    \n",
    "    for p in params:\n",
    "        p.update(kwargs)\n",
    "        \n",
    "    evaluate = map(lambda m, p, x, y: getRandom(m, p, x[0], x[1], y[0], y[1]), cnn, params, train, test)\n",
    "    \n",
    "    return evaluate\n",
    "        \n",
    "    \n",
    "'''\n",
    "Get the data and preproess, load embedding models, convert train and test data into embeddings. \n",
    "Perform parameter tuning if tune==True. Fit model and call NN classifiers\n",
    "'''\n",
    "def getNeuralClassifiction(libraries, names, paths, textTrain, labelsTrain, textTest, labelsTest, methods, \n",
    "    tokenTypes=[False], cleanSchedule=None, models=None, retrain=False, tune=False, params=None):\n",
    "    \n",
    "    tokensTrain = list(map(initializePreProcessing, [textTrain]*len(names), tokenTypes, cleanSchedule))\n",
    "    tokensTest = list(map(initializePreProcessing, [textTest]*len(names), tokenTypes, cleanSchedule))\n",
    "    \n",
    "    if models == None:\n",
    "        models = list(map(lambda x, y: em.EmbeddingModel(library=x, name=y), libraries, names))\n",
    "        for m, p in zip(models, paths):\n",
    "            m.load(p)\n",
    "    else:\n",
    "        models = list(map(lambda x, y: em.EmbeddingModel(model=x, name=y), models, names))\n",
    "    \n",
    "    wIndicies = [embed.getWordIndex() for embed in models]\n",
    "    embeddings = [embed.model.wv.vectors for embed in models]\n",
    "    \n",
    "    train = list(map(lambda x, y, z: getFeatures(x, y, z.tolist(), labelsTrain), models, wIndicies, tokensTrain))\n",
    "    test =  list(map(lambda x, y, z: getFeatures(x, y, z.tolist(), labelsTest), models, wIndicies, tokensTest))\n",
    "\n",
    "    seqDims = [len(x[0][0]) for x in train]\n",
    "    vocabDims = [len(w) for w in wIndicies]\n",
    "    embedDims = [len(m[0]) for m in embeddings]\n",
    "    \n",
    "    if tune:\n",
    "        evaluate = getTune(params, embeddings=embeddings, seqDim=[seqDims[0]], vocabDim=[vocabDims[0]], embedDim=[embedDims[0]])\n",
    "    else:\n",
    "        models = map(lambda m, x, y, z, k: m(x,y,z,k, train=retrain), methods, embeddings, vocabDims, embedDims, seqDims)\n",
    "        evaluate = map(lambda m, x, y: fitModel(m, x[0], x[1], y[0], y[1]), models, train, test)\n",
    "    \n",
    "    return list(evaluate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get the average results for each metric across each classifier\n",
    "'''\n",
    "\n",
    "def getAverage(results):\n",
    "    results = list(zip(*results))\n",
    "    results = [[np.array(metric).mean() for metric in list(zip(*model))]for model in results]\n",
    "    \n",
    "    return results\n",
    "\n",
    "'''\n",
    "evaluate each model and save down in csv\n",
    "'''\n",
    "def getEvalTrials(results, names, path):\n",
    "\n",
    "    results = getAverage(results)\n",
    "    \n",
    "    accuracy = [results[m][1] for m in range(len(results))]\n",
    "    precision = [results[m][2] for m in range(len(results))]\n",
    "    recall = [results[m][3] for m in range(len(results))]\n",
    "    f1 = [results[m][4] for m in range(len(results))]\n",
    "        \n",
    "\n",
    "    evalDict = {'accuracy':accuracy, 'precision':precision, 'recall':recall, 'f1':f1}\n",
    "    evalDf = pd.DataFrame(evalDict, index=names)\n",
    "\n",
    "    evalDf.to_csv(path + 'Summary' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "initiate getNeuralClassification function and then call and run for number of trials.\n",
    "'''\n",
    "def getSchizResults(libraries, modelNames, paths, textSchiz1, labelsSchiz1, \n",
    "                                 textSchiz2, labelsSchiz2, methods, token, cleanSchedule, resultsPath, models=None, retrain=False):\n",
    "    \n",
    "    tokenTypes = utility.getTokenTypes(token, modelNames)\n",
    "    x = functools.partial(getNeuralClassifiction, libraries, modelNames, paths, textSchiz1, labelsSchiz1, \n",
    "                                 textSchiz2, labelsSchiz2, methods, tokenTypes, cleanSchedule, models, retrain=False)\n",
    "    \n",
    "    resultsSchiz = [x() for i in range(trials)]\n",
    "    \n",
    "    getEvalTrials(resultsSchiz, models, resultsPath)\n",
    "    \n",
    "    return resultsSchiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoticon Models - All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform classification for all data and data with emoticons removed. Logic follows the same as 04 Machine learning. Do word and characters each for LSTM and CNN and then do Glove word once for each as well. First lets look at words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emWordModels = ['w2vWCBSchiz', 'w2vWCBEmSchiz', 'w2vWSGSchiz', 'w2vWSGEmSchiz', 'ftWCBSchiz', 'ftWCBEmSchiz', 'ftWSGSchiz', 'ftWSGEmSchiz']\n",
    "emCharModels = ['w2vCCBSchiz', 'w2vCCBEMSchiz', 'w2vCSGSchiz', 'w2vCSGEmSchiz', 'ftCCBSchiz', 'ftCCBEMSchiz', 'ftCSGSchiz', 'ftCSGEmSchiz']\n",
    "path1 = ['embeddings/Word2Vec/schiz/00 emoticons/all/']\n",
    "path2 = ['embeddings/FastText/schiz/00 emoticons/all/']\n",
    "paths = (path1*4) + (path2*4)\n",
    "libraries = [Word2Vec, Word2Vec, Word2Vec, Word2Vec, FastText, FastText, FastText, FastText]\n",
    "cleanSchedule = [m1EClean, m2EClean, m1EClean, m2EClean]*2\n",
    "paramGrid = dict(dropout=dropout, reDropout=dropout, neurons=neurons, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Emoticons\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Emoticons\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Emoticons\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Emoticons\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Emoticons\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Emoticons\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Emoticons\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Emoticons\n",
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'embeddings/Word2Vec/schiz/00 emoticons/all/w2vWCBSchiz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1ef87d3562c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmethods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetLSTMModel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memWordModels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m resultLSTMWordSchiz = getSchizResults(libraries, emWordModels, paths, textSchiz1, labelsSchiz1, \n\u001b[0;32m----> 5\u001b[0;31m                                  textSchiz2, labelsSchiz2, methods, wordToken, cleanSchedule, resultsPath, retrain=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-7608d3b02ae3>\u001b[0m in \u001b[0;36mgetSchizResults\u001b[0;34m(libraries, modelNames, paths, textSchiz1, labelsSchiz1, textSchiz2, labelsSchiz2, methods, token, cleanSchedule, resultsPath, models, retrain)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                  textSchiz2, labelsSchiz2, methods, tokenTypes, cleanSchedule, models, retrain=False)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresultsSchiz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mgetEvalTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresultsSchiz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresultsPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-7608d3b02ae3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                  textSchiz2, labelsSchiz2, methods, tokenTypes, cleanSchedule, models, retrain=False)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresultsSchiz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mgetEvalTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresultsSchiz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresultsPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8ef9c08e3ad2>\u001b[0m in \u001b[0;36mgetNeuralClassifiction\u001b[0;34m(libraries, names, paths, textTrain, labelsTrain, textTest, labelsTest, methods, tokenTypes, cleanSchedule, models, retrain, tune, params)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibraries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/King's/Mental Health/Final Workbooks/scripts/embeddings.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetSimilar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \"\"\"\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m     \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'embeddings/Word2Vec/schiz/00 emoticons/all/w2vWCBSchiz'"
     ]
    }
   ],
   "source": [
    "#LSTM Model\n",
    "resultsPath = 'results/neural embeddings/nn/all/lstm/'\n",
    "methods = [getLSTMModel]*len(emWordModels)\n",
    "resultLSTMWordSchiz = getSchizResults(libraries, emWordModels, paths, textSchiz1, labelsSchiz1, \n",
    "                                 textSchiz2, labelsSchiz2, methods, wordToken, cleanSchedule, resultsPath, retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Model\n",
    "resultsPath = 'results/neural embeddings/nn/all/cnn/'\n",
    "methods = [getCNNModel]*len(emWordModels)\n",
    "resultCNNWordSchiz = getSchizResults(libraries, emWordModels, paths, textSchiz1, labelsSchiz1, \n",
    "                                 textSchiz2, labelsSchiz2, methods, wordToken, cleanSchedule, resultsPath, retrain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform for Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloveModels = [KeyedVectors.load_word2vec_format('embeddings/glove/glove200', binary=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPath = 'results/neural embeddings/nn/all/cnn/glove'\n",
    "cleanSchedule = [m2EClean]\n",
    "paths = ['']\n",
    "libraries=['']\n",
    "methods = [getCNNModel]*len(gloveModels)\n",
    "\n",
    "resultCNNWordSchiz = getSchizResults(libraries, gloveModels, paths, textSchiz1, labelsSchiz1, \n",
    "                textSchiz2, labelsSchiz2, methods, wordToken, cleanSchedule, resultsPath, models=gloveModels, retrain=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPath = 'results/neural embeddings/nn/all/cnn/glove'\n",
    "cleanSchedule = [m2EClean]\n",
    "\n",
    "paths = ['']\n",
    "libraries=['']\n",
    "methods = [getLSTM2]*len(gloveModels)\n",
    "\n",
    "resultCNNWordSchiz = getSchizResults(libraries, gloveModels, paths, textSchiz1, labelsSchiz1, \n",
    "                textSchiz2, labelsSchiz2, methods, wordToken, cleanSchedule, resultsPath, models=gloveModels, retrain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform classification on character embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "emCharModels = ['w2vCCBSchiz', 'w2vCCBEMSchiz', 'w2vCSGSchiz', 'w2vCSGEmSchiz', 'ftCCBSchiz', 'ftCCBEMSchiz', 'ftCSGSchiz', 'ftCSGEmSchiz']\n",
    "path1 = ['embeddings/Word2Vec/schiz/00 emoticons/all/']\n",
    "path2 = ['embeddings/FastText/schiz/00 emoticons/all/']\n",
    "paths = (path1*4) + (path2*4)\n",
    "libraries = [Word2Vec, Word2Vec, Word2Vec, Word2Vec, FastText, FastText, FastText, FastText]\n",
    "cleanSchedule = [m1EClean, m2EClean, m1EClean, m2EClean]*2\n",
    "paramGrid = dict(dropout=dropout, reDropout=dropout, neurons=neurons, optimizer=optimizer)\n",
    "\n",
    "tokenTypes = utility.getTokenTypes(charToken, emCharModels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Model\n",
    "resultsPath = 'results/neural embeddings/nn/all/lstm/char'\n",
    "methods = [getLSTMModel]*len(emCharModels)\n",
    "resultLSTMCharSchiz = getSchizResults(libraries, emCharModels, paths, textSchiz1, labelsSchiz1, \n",
    "                                 textSchiz2, labelsSchiz2, methods, charToken, cleanSchedule, resultsPath, retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Model\n",
    "resultsPath = 'results/neural embeddings/nn/all/cnn/char'\n",
    "methods = [getCNNModel]*len(emCharModels)\n",
    "resultCNNWordSchiz = getSchizResults(libraries, emCharModels, paths, textSchiz1, labelsSchiz1, \n",
    "                                 textSchiz2, labelsSchiz2, methods, charToken, cleanSchedule, resultsPath, retrain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoticon Models - emoticon only models - Not used in final Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at just emoticon tweets\n",
    "emWordModels = ['w2vemWCBSchiz', 'w2vemWCBEmSchiz', 'ftemWCBSchiz', 'ftemWCBEmSchiz']\n",
    "emCharModels = ['w2vemCCBSchiz', 'w2vemCCBEMSchiz', 'ftemCCBSchiz', 'ftemCCBEMSchiz']\n",
    "path1 = ['embeddings/Word2Vec/schiz/00 emoticons/emoticonOnly/']\n",
    "path2 = ['embeddings/FastText/schiz/00 emoticons/emoticonOnly/']\n",
    "emWordModels = getFilePath(path1, path2, emWordModels)\n",
    "emCharModels = getFilePath(path1, path2, emCharModels)\n",
    "libraries = [Word2Vec, Word2Vec, Word2Vec, Word2Vec, FastText, FastText, FastText, FastText]\n",
    "cleanSchedule = [m1EClean, m2EClean] * 2\n",
    "charParametersLst = getParameters(charParameters, emWordModels)\n",
    "wordParametersLst = getParameters(wordParameters, emWordModels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider word tokens\n",
    "tokenTypes = utility.getTokenTypes(wordToken, emWordModels)\n",
    "resultWordSchiz = getNeuralClassifiction(libraries, emWordModels, paths, textSchiz1, labelsSchiz1, textSchiz2, labelsSchiz2, methods, tokenTypes, cleanSchedule, retrain=False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider char tokens\n",
    "tokenTypes = utility.getTokenTypes(charToken, emWordModels)\n",
    "resultCharSchiz = getNeuralClassifiction(libraries, emWordModels, paths, textSchiz1, labelsSchiz1, textSchiz2, labelsSchiz2, methods, tokenTypes, cleanSchedule, retrain=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase Models - All    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we focus on models with capitalized words. Using M2lClean we convert them to lowercase. This follows the same pipeline as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at All tweets. ##### Need to get embedding models for Skip gram ####\n",
    "lWordModels = ['w2vWCBlSchiz', 'w2vWCBSchiz', 'w2vWSGlSchiz', 'w2vWSGSchiz']\n",
    "lCharModels = []\n",
    "path1 = ['embeddings/Word2Vec/schiz/01 lowercase/all/']\n",
    "paths = path1*4\n",
    "cleanSchedule = [m2LClean, m1LClean, m2LClean, m1LClean]\n",
    "libraries = [Word2Vec, Word2Vec, Word2Vec, Word2Vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPath = 'results/neural embeddings/nn/lower/lstm/word'\n",
    "tokenTypes = utility.getTokenTypes(wordToken, lWordModels)\n",
    "methods = [getLSTMModel]*len(lWordModels)\n",
    "resultLSTMCharSchiz = getSchizResults(libraries, lWordModels, paths, textSchiz1, labelsSchiz1, \n",
    "                                 textSchiz2, labelsSchiz2, methods, wordToken, cleanSchedule, resultsPath, retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPath = 'results/neural embeddings/nn/lower/cnn/word'\n",
    "methods = [getCNNModel]*len(lWordModels)\n",
    "resultCNNWordSchiz = getSchizResults(libraries, lWordModels, paths, textSchiz1, labelsSchiz1, \n",
    "                                 textSchiz2, labelsSchiz2, methods, wordToken, cleanSchedule, resultsPath, retrain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase Models - lowercase tweets only - Not used in the final Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at just lowercase tweets\n",
    "lOnlyWordModels = ['w2vlWCBlSchiz', 'w2vlWCBSchiz', 'w2vlWSGlSchiz', 'w2vlWSGSchiz', 'ftlWCBlSchiz', 'ftlWCBSchiz', 'ftlWSGlSchiz', 'ftlWSGSchiz']\n",
    "lOnlyCharModes = ['']\n",
    "\n",
    "path1 = ['embeddings/Word2Vec/schiz/01 lowercase/lowerOnly/']\n",
    "path2 = ['embeddings/FastText/schiz/01 lowercase/lowerOnly/']\n",
    "\n",
    "lOnlyWordModels = getFilePath(path1, path2, lWordModels)\n",
    "lOnlyCharModels  getFilePath(path1, path2, lCharModels)\n",
    "\n",
    "cleanSchedule = [m2LClean, m1LClean, m2LClean, m1LClean]*2\n",
    "\n",
    "charParametersLst = getParameters(charParameters, emWordModels)\n",
    "wordParametersLst = getParameters(wordParameters, emWordModels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider word tokens\n",
    "tokenTypes = utility.getTokenTypes(wordToken, emWordModels)\n",
    "resultWordSchiz = getNeuralClassifiction(libraries, emWordModels, paths, textSchiz1, labelsSchiz1, textSchiz2, labelsSchiz2, methods, tokenTypes, cleanSchedule, retrain=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider char tokens\n",
    "tokenTypes = utility.getTokenTypes(charToken, emWordModels)\n",
    "resultCharSchiz = getNeuralClassifiction(libraries, emCharModels, paths, textSchiz1, labelsSchiz1, textSchiz2, labelsSchiz2, methods, tokenTypes, cleanSchedule, retrain=False )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Datasets Not in this paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emCombWordModels = ['w2vWCBAll', 'w2vWCBEmAll', 'w2vWSGAll', 'w2vWSGEmAll', 'ftWCBAll', 'ftWCBEmAll', 'ftWSGAll', 'ftWSGEmAll']\n",
    "emCombCharModels = ['w2vCCBAll', 'w2vCCBEMAll', 'w2vCSGAll', 'w2vCSGEmAll', 'ftCCBAll', 'ftCCBEMAll', 'ftCSGAll', 'ftCSGEmAll']\n",
    "\n",
    "path1 = ['embeddings/Word2Vec/combined/']\n",
    "path2 = ['embeddings/FastText/combined/']\n",
    "\n",
    "emCombWordModels = getFilePath(path1, path2, emCombWordModels)\n",
    "emCombCharModels = getFilePath(path1, path2, emCombCharModels)\n",
    "\n",
    "cleanSchedule = [m1EClean, m2EClean, m1EClean, m2EClean] * 2\n",
    "\n",
    "charParametersLst = getParameters(charParameters, emCombCharModels)\n",
    "wordParametersLst = getParameters(wordParameters, emCombWordModels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider word tokens\n",
    "tokenTypes = utility.getTokenTypes(wordToken, emWordModels)\n",
    "resultWordSchiz = getNeuralClassifiction(libraries, emWordModels, paths, textSchiz1, labelsSchiz1, textSchiz2, labelsSchiz2, methods, tokenTypes, cleanSchedule, retrain=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider char tokens\n",
    "tokenTypes = utility.getTokenTypes(charToken, emWordModels)\n",
    "resultCharSchiz = getNeuralClassifiction(libraries, emCharModels, paths, textSchiz1, labelsSchiz1, textSchiz2, labelsSchiz2, methods, tokenTypes, cleanSchedule, retrain=False )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
