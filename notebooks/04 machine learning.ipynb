{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains main code for binary classification using SVM, RandomForest, GradientBoost and KNN\n",
    "using saved down embedding models. Makes use of ml, evaluation, preprocessing, feature_engineering and utlity scripts. Again we set up a pipeline so we can train multiple models at once and use similar logic to 03 Embeddings notebook with NLP preprocessing happening first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhYYpnh8P-eN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from emoji import UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "08ZDjFjvP-eS",
    "outputId": "795f5a3a-adef-473f-e9f5-93cd1b34bb46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gregoryverghese/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gregoryverghese/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gregoryverghese/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gregoryverghese/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import utility\n",
    "import evaluation as ev\n",
    "import ml\n",
    "import preprocessing as pp\n",
    "import ml_config as mlc\n",
    "import feature_engineering as fe\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qIrBzDEgP-ef"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set global parameters and file paths\n",
    "'''\n",
    "trials = 1\n",
    "cbowTrain = 0\n",
    "sgTrain = 1\n",
    "wordToken = False\n",
    "charToken = True\n",
    "m1EClean=['Tokens', 'Lemma', 'Stopwords', 'Phrases', 'Emoticons']\n",
    "m2EClean=['Tokens', 'Lemma', 'Stopwords', 'Phrases']\n",
    "m1LClean=['Tokens', 'Lemma', 'Stopwords', 'Phrases', 'Lowercase']\n",
    "m2LClean=['Tokens', 'Lemma', 'Stopwords', 'Phrases']\n",
    "gloveInFileName = 'data/glove/glove.twitter.27B.'\n",
    "gloveOutFileName = 'gensim_glove_vectors_'\n",
    "gloveDim = ['25d.txt', '50d.txt', '100d.txt', '200d.txt']\n",
    "fileNameStig = 'data/dataOut/stigma/annFinalStig.csv'\n",
    "fileNameSchiz1 = 'data/dataOut/schiz/annFinalSchiz_1.csv'\n",
    "fileNameSchiz2 = 'data/dataOut/schiz/annFinalSchiz_2.csv'\n",
    "abbreviations = pd.read_csv('data/other/abbreviations.csv')['Abbreviation'].tolist()\n",
    "abbreviations = [str(a).strip() for a in abbreviations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "colab_type": "code",
    "id": "ot2FT71IP-ej",
    "outputId": "d03cc832-77c3-4007-957a-ee556cf9bcc6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Read in three datasets. 1. Stigma data 2. 1000 labelled schizophrenia data 3. 500 labelled schizophrenia data\n",
    "'''\n",
    "socialDf = pd.read_csv(fileNameStig, encoding='utf-8')\n",
    "textStigma = socialDf['Tweet']\n",
    "labelsStigma = socialDf['Classification']\n",
    "\n",
    "socialDf = pd.read_csv(fileNameSchiz1, encoding='utf-8')\n",
    "textSchiz1 = socialDf['Tweet']\n",
    "labelsSchiz1 = socialDf['Classification']\n",
    "\n",
    "socialDf = pd.read_csv(fileNameSchiz2, encoding='utf-8')\n",
    "textSchiz2 = socialDf['Tweet']\n",
    "labelsSchiz2 = socialDf['Classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpvew1vv0bK2"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parameter settings stored in ml_config file. Parameters have \n",
    "been tuned using RandomSearch from Scikit learn package\n",
    "'''\n",
    "rfRandomGrid = mlc.rfRandomGrid\n",
    "gbRandomGrid = mlc.gbRandomGrid\n",
    "knnRandomGrid = mlc.knnRandomGrid\n",
    "svmRandomGrid = mlc.svmRandomGrid\n",
    "\n",
    "wordParameters = mlc.wordParameters\n",
    "charParameters = mlc.charParameters\n",
    "\n",
    "randomGrids = [rfRandomGrid, gbRandomGrid, svmRandomGrid, knnRandomGrid]\n",
    "methods = ['RandomForest', 'GradientBoost', 'SVM', 'KNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Word2Vec.load('embeddings/Word2Vec/stigma/w2vWCBEmStig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5962"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = a.wv.vocab.items()\n",
    "len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "b = textSchiz2.apply(lambda x: nltk.word_tokenize(x))\n",
    "c = b.tolist()\n",
    "x = [t for s in c for t in s if t not in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11900"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "31YfobnnP-e2"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "calls preprocessing class in pp script\n",
    "'''\n",
    "def initializePreProcessing(text, tokenType, cleanMethods=['Tokens', 'Lemma', 'Stopwords', 'Phrases', 'Emoticons']):\n",
    "\n",
    "    social = pp.SocialPreProcessing(text, tokenType)\n",
    "    socialClean = social.clean(cleanMethods)\n",
    "    \n",
    "    return socialClean\n",
    "\n",
    "\n",
    "'''\n",
    "calculates average embedding vector for a sentence/post\n",
    "'''\n",
    "def getAvgEmbeddings(sentence, model):\n",
    "    \n",
    "    totalEmbedding = sum([model[tok] for tok in sentence if tok in model])\n",
    "    avgEmbedding = totalEmbedding/float(len(sentence))\n",
    "    \n",
    "    return np.array(avgEmbedding)\n",
    "\n",
    "\n",
    "'''\n",
    "gets embedding data for all posts\n",
    "'''\n",
    "def getEmbeddingData(model, posts):\n",
    "    \n",
    "    numFeatures = model.vector_size\n",
    "    postFeatureVec = np.zeros((len(posts), numFeatures), dtype='float32')\n",
    "    \n",
    "    for i in range(len(posts)):\n",
    "        postFeatureVec[i] = getAvgEmbeddings(posts[i], model)\n",
    "    \n",
    "    return postFeatureVec\n",
    "\n",
    "\n",
    "'''\n",
    "initializes baseML class from ml script and calls passed \n",
    "classifiers with corresponding parameters\n",
    "'''\n",
    "def initializeML(features, labels, methods, parameters):\n",
    "    \n",
    "    mLearn = ml.baseML(features, labels)\n",
    "    mLearn.classifiers = [getattr(mLearn,'get'+f)(p) for f, p in zip(methods, parameters)]\n",
    "    return mLearn.getAllPredictions(methods, parameters)\n",
    "\n",
    "\n",
    "'''\n",
    "trains tf-idf matrix on train/test data and\n",
    "concatenates with corresponding embedding data\n",
    "\n",
    "'''\n",
    "def initializeFeatures(text, xTrainSets):\n",
    "    \n",
    "    text = text.to_frame(name='tweets')\n",
    "    fEng = fe.FeatureEngineering(text)\n",
    "    fBoost = fEng.getFeatures('tweets')\n",
    "    fBoost = fBoost.drop(columns=['tweets'])\n",
    "    \n",
    "    tfidfVect = TfidfVectorizer(encoding='utf-8', lowercase=False, stop_words='english', analyzer='word')\n",
    "    tfidf = tfidfVect.fit(text['tweets'])\n",
    "    tfidf = tfidf.transform(text['tweets'])\n",
    "    tfidfDf = pd.DataFrame(tfidf.toarray(), columns=tfidfVect.get_feature_names())\n",
    "    features = pd.concat([fBoost.reset_index(drop=True), tfidfDf], axis=1)\n",
    "    \n",
    "    xTrainSets = [np.concatenate((features.to_numpy(), x), axis=1) for x in xTrainSets]\n",
    "    \n",
    "    return xTrainSets\n",
    "\n",
    "\n",
    "'''\n",
    "get classification for each of the passed embedding models\n",
    "'''\n",
    "def getMLresult(methods, parameters, text, labels, tokenTypes, cleanSchedule, modelFileNames=None, models=None, featureBoost=False):\n",
    "     \n",
    "    print(wordToken)\n",
    "        \n",
    "    modelNum = len(cleanSchedule)\n",
    "    \n",
    "    if models == None:\n",
    "        models = map(Word2Vec.load, modelFileNames)\n",
    "       \n",
    "    trainTexts = map(initializePreProcessing, [text]*modelNum, tokenTypes, cleanSchedule)\n",
    "    \n",
    "    xTrainSets = map(lambda x, y: getEmbeddingData(x, y.tolist()), models, trainTexts)\n",
    "    \n",
    "    if featureBoost:\n",
    "        f = initializeFeatures(text, xTrainSets)\n",
    "        \n",
    "    mlResults = map(initializeML, xTrainSets, [labels]*modelNum, [methods]*modelNum, parameters)\n",
    "    \n",
    "    return mlResults\n",
    "\n",
    "\n",
    "'''\n",
    "begins parameter tuning for each of the passed classifiers\n",
    "'''\n",
    "def initializeTuning(methods, randomGrids, features, labels):\n",
    "    \n",
    "    parameters = [{}]*len(methods)\n",
    "    \n",
    "    mLearn = ml.baseML(features, labels)\n",
    "    mLearn.classifiers = [getattr(mLearn,'get'+f)(p) for f, p in zip(methods, parameters)]\n",
    "    \n",
    "    parameters = mLearn.getHyperParams(randomGrids)\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "'''\n",
    "sets up models and data ready for parameter tuning. If featureBoost,\n",
    "then concatenate tf-idf with embedding data\n",
    "'''\n",
    "def getMLParameters(methods, randomGrids, features, labels, tokenTypes, cleanSchedule, modelNames, models, featureBoost):\n",
    "\n",
    "    modelNum = len(cleanSchedule)\n",
    "    \n",
    "    if models == None:\n",
    "        models = map(Word2Vec.load, modelNames)\n",
    "    \n",
    "    trainTexts = map(initializePreProcessing, [features]*modelNum, tokenTypes, cleanSchedule)\n",
    "    xTrainSets = map(lambda x, y: getEmbeddingData(x, y.tolist()), models, trainTexts)\n",
    "    \n",
    "    \n",
    "    if featureBoost:\n",
    "        xTrainSets = initializeFeatures(text, xTrainSets)\n",
    "        \n",
    "    mlParams = map(initializeTuning, [methods]*modelNum, [randomGrids]*modelNum, xTrainSets, [labels]*modelNum)\n",
    "    \n",
    "    return mlParams\n",
    "\n",
    "\n",
    "'''\n",
    "get best parameters from tuning process then pass to getMLResults\n",
    "with test data for classificaton\n",
    "'''\n",
    "def getClassification(trainText, trainLabels, testText, testLabels, randomGrids, cleanSchedule, token, methods, modelNames=None, models=None, featureBoost=False):\n",
    "    \n",
    "    tokenTypes = utility.getTokenTypes(token, emWordModels)\n",
    "    tunedParams = getMLParameters(methods, randomGrids, trainText, trainLabels, tokenTypes, cleanSchedule, modelNames, models, featureBoost)\n",
    "    parameters = [[p.best_params_ for p in m] for m in tunedParams]\n",
    "    results = getMLresults(methods, parameters, testText, testLabels, tokenTypes, cleanSchedule, modelNames, models, featureBoost)\n",
    "    \n",
    "    return (tunedParams, results) \n",
    "\n",
    "'''\n",
    "get ML results for n trials. Save down average results in csv\n",
    "'''\n",
    "def getResults(modelPaths, modelNames, token, cleanSchedule, parametersLst, textSchiz2, labelsSchiz2, resultsPath, featureBoost=None):\n",
    "    \n",
    "    tokenTypes = utility.getTokenTypes(token, modelNames)\n",
    "    \n",
    "    x = functools.partial(getMLresult, methods, parametersLst, textSchiz2, labelsSchiz2, tokenTypes, cleanSchedule, modelPaths, featureBoost=False)\n",
    "    resultsSchiz = [x() for i in range(trials)]\n",
    "    \n",
    "    getEvalTrials(resultsSchiz, modelNames, resultsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "return filePath\n",
    "'''\n",
    "def getFilePath(paths, names):\n",
    "    return [p + n for p, n in zip(paths, names)]\n",
    "\n",
    "'''\n",
    "get for each metric for model across the number of trials\n",
    "'''\n",
    "def getAverage(results, metric):\n",
    "\n",
    "    classifiers = zip(*results)\n",
    "    average = [np.array([trial[metric].mean() for trial in clf]).mean() for clf in classifiers]\n",
    "\n",
    "    return average\n",
    "\n",
    "\n",
    "'''\n",
    "get average for each metric for each model and save each model \n",
    "down in a separate csv file.\n",
    "'''\n",
    "def getEvalTrials(results, names, path):\n",
    "\n",
    "    results = zip(*results)\n",
    "    \n",
    "\n",
    "    for m in range(len(results)):\n",
    "        accuracy = getAverage(results[m], 'test_accuracy')\n",
    "        precision = getAverage(results[m], 'test_precision')\n",
    "        recall = getAverage(results[m], 'test_recall')\n",
    "        f1 = getAverage(results[m], 'test_f1')\n",
    "        roc_auc = getAverage(results[m], 'test_roc_auc')\n",
    "\n",
    "        evalDict = {'accuracy':accuracy, 'precision':precision, 'recall':recall, 'f1':f1, 'roc_auc':roc_auc}\n",
    "        evalDf = pd.DataFrame(evalDict, index=methods)\n",
    "\n",
    "        evalDf.to_csv(path + names[m] + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9XN6MCnAR-ik"
   },
   "source": [
    "## Stigma Mental Health - Not used in this paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzKjQ4NXQG5L"
   },
   "outputs": [],
   "source": [
    "modelNames = ['w2vWCBStig', 'w2vWCBEmStig', 'ftWSGStig', 'ftWSGEmStig']\n",
    "path1 = ['embeddings/Word2Vec/stigma/']\n",
    "path2 = ['embeddings/FastText/stigma/'] \n",
    "models = utility.getFilePath(path1, path2, modelNames)\n",
    "cleanSchedule = [m2EClean, m2EClean] *2\n",
    "tokenTypes = [False, False, False, False]\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(textStigma, labelsStigma, test_size=0.3, random_state=42)\n",
    "wordParametersLst = utility.getParameters(wordParameters, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "colab_type": "code",
    "id": "RIkFxGk_Z03R",
    "outputId": "cddd914d-8a26-414b-9e48-b99194143270"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregoryverghese/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/gregoryverghese/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#modelStig = getClassification(xTrain, yTrain, xTest, yTest, randomGrids, cleanSchedule, tokenTypes, methods,  modelNames)\n",
    "#bestParamsStig = getBestParameters(modelStig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsStig = getMLresult(methods, wordParametersLst, xTest, yTest, tokenTypes, cleanSchedule, modelNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmOK55YVygto"
   },
   "source": [
    "## Schizophrenia stigma data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we perform the classification task for each of the models. We do it for word data, and char data. Word2Vec \n",
    "and FastText do it in the same pipeline. We perform word classification for Glove separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghiuaRheP-e7"
   },
   "source": [
    "### Emoticon Models - All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets consider models with all data and emoticons stripped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanSchedule = [m2EClean, m2EClean]\n",
    "resultsPath = 'results/neural embeddings/all/'\n",
    "path1 = ['embeddings/Word2Vec/schiz/00 emoticons/all/']\n",
    "path2 = ['embeddings/FastText/schiz/00 emoticons/all/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Schizophrenia models, focus on emoticons 1.Word 2.Character 3.Glove\n",
    "'''\n",
    "\n",
    "'''\n",
    "get word models\n",
    "'''\n",
    "def getSchizWordData(cleanSchedule, resultsPath, path1, path2):\n",
    "\n",
    "    #emWordModels = ['w2vWCBSchiz', 'w2vWCBEmSchiz', 'w2vWSGSchiz', 'w2vWSGEmSchiz', 'ftWCBSchiz', 'ftWCBEmSchiz', 'ftWSGSchiz', 'ftWSGEmSchiz']\n",
    "    emWordModels = ['w2vWSGEmSchiz', 'ftWSGEmSchiz']\n",
    "    emWordPaths = utility.getFilePath(path1, path2, emWordModels)\n",
    "    wordParametersLst = utility.getParameters(wordParameters, emWordPaths)\n",
    "    getResults(emWordPaths, emWordModels, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, textSchiz2, labelsSchiz2, resultsPath)\n",
    "\n",
    "'''\n",
    "get character models\n",
    "'''  \n",
    "def getSchizCharData(cleanSchedule, resultsPath, path1, path2):\n",
    "\n",
    "    emCharModels = ['w2vCCBSchiz', 'w2vCCBEMSchiz', 'w2vCSGSchiz', 'w2vCSGEmSchiz', 'ftCCBSchiz', 'ftCCBEMSchiz', 'ftCSGSchiz', 'ftCSGEmSchiz']\n",
    "    emCharPaths = utility.getFilePath(path1, path2, emCharModels)\n",
    "    charParametersLst = utility.getParameters(charParameters, emCharPaths)\n",
    "    getResults(emCharPaths, emCharModels, charToken, cleanSchedule, \n",
    "                                        charParametersLst, textSchiz2, labelsSchiz2, resultsPath)\n",
    "\n",
    "'''\n",
    "get Glove models\n",
    "'''\n",
    "def getSchizGlove(cleanSchedule, resultsPath, path1, path2):\n",
    "    \n",
    "    gloveModels = ['glove200']\n",
    "    cleanSchedule = [m2EClean]\n",
    "    gloveModels = [KeyedVectors.load_word2vec_format('embeddings/glove/glove200', binary=False)]\n",
    "    gloveParametersLst = utility.getParameters(wordParameters, GloveModels)\n",
    "    getResults(emCharPaths, gloveModels, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, textSchiz2, labelsSchiz2, resultsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSchizWordData(cleanSchedule, resultsPath, path1, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSchizCharData(cleanSchedule, resultsPath, path1, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSchizGlove(cleanSchedule, resultsPath, path1, path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji WorkAround due to NLTK tokenizer problem (see README.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way as '03 Embeddings' we use the Spacey Package as a workaround the NLTK emoji tokenize issue. First we have to load in the csv file that has been tokenized by Spacey and then convert it into a tractable form that captures this tokenization. Note in the M1EClean list we do not have the Tokenize cleaning command becasue it is already tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileEmoji = 'data/dataOut/schiz/emoji/annSchiz2.csv'\n",
    "textEmoji = pp.getFile(fileEmoji, 'Tweet')\n",
    "textEmoji = textEmoji.apply(lambda x: x[1:len(x)-1])\n",
    "textEmoji = textEmoji.apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tEmoji = textEmoji.tolist()\n",
    "tEmoji = [[tok[1:len(tok)-1] for tok in sent] for sent in tEmoji]\n",
    "textEmoji = pd.Series(tEmoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileEmojiChar = 'data/dataOut/schiz/emoji/annSchizChar2.csv'\n",
    "textEmojiChar = pp.getFile(fileEmojiChar, 'Tweet')\n",
    "textEmojiChar = textEmojiChar.apply(lambda x: x[1:len(x)-1])\n",
    "textEmojichar = textEmojiChar.apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1EClean=['Lemma', 'Stopwords', 'Phrases', 'Emoticons']\n",
    "m2EClean=['Lemma', 'Stopwords', 'Phrases']\n",
    "cleanSchedule = [m1EClean, m2EClean, m1EClean, m2EClean]*2\n",
    "resultsPath = 'results/neural embeddings/emojiAll/'\n",
    "path1 = ['embeddings/Word2Vec/schiz/02 emoticons2/']\n",
    "path2 = ['embeddings/FastText/schiz/02 emoticons2/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get word models\n",
    "'''\n",
    "\n",
    "def getSchizWordData2(text, labels, cleanSchedule, resultsPath, path1, path2):\n",
    "\n",
    "    emWordModels = ['w2vemWCBSchiz', 'w2vemWCBEmSchiz', 'w2vemWSGSchiz', 'w2vemWSGEmSchiz', 'ftemWCBSchiz', 'ftemWCBEmSchiz', 'ftemWSGSchiz', 'ftemWSGEmSchiz']\n",
    "    emWordPaths = utility.getFilePath(path1, path2, emWordModels)\n",
    "    wordParametersLst = utility.getParameters(wordParameters, emWordPaths)\n",
    "    getResults(emWordPaths, emWordModels, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, text, labels, resultsPath)\n",
    "\n",
    " '''\n",
    " get character models\n",
    " '''   \n",
    "def getSchizCharData2(text, labels, cleanSchedule, resultsPath, path1, path2):\n",
    "\n",
    "    emCharModels = ['w2vemCCBSchiz', 'w2vemCCBEMSchiz', 'w2vemCSGSchiz', 'w2vemCSGEmSchiz', 'ftemCCBSchiz', 'ftemCCBEMSchiz', 'ftemCSGSchiz', 'ftemCSGEmSchiz']\n",
    "    emCharPaths = utility.getFilePath(path1, path2, emCharModels)\n",
    "    charParametersLst = utility.getParameters(charParameters, emCharPaths)\n",
    "    getResults(emCharPaths, emCharModels, charToken, cleanSchedule, \n",
    "                                        charParametersLst, text, labelsSchiz2, resultsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSchizWordData2(textEmoji, labelsSchiz2, cleanSchedule, resultsPath, path1, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSchizCharData2(textEmoji, labelsSchiz2, cleanSchedule, resultsPath, path1, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSchizGlove2(cleanSchedule, resultsPath, path1, path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next part we only consider tweets containing the emoticons or capitalized word in the classification, this is a much smaller number. 56 for emoticons and ca. 3000 for capitalixed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "'''\n",
    "return emoticon only tweets\n",
    "'''\n",
    "def getEmoticonText(tokens, labels, emojis):\n",
    "\n",
    "    emojiLines = list(map(lambda x: any(i in emojis for i in x), tokens))\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    tokens = np.array(tokens)\n",
    "    \n",
    "    emojiLines = np.array(emojiLines)\n",
    "    \n",
    "    tokens = tokens[emojiLines]\n",
    "    labels = labels[emojiLines]\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "'''\n",
    "return tweets only containiong capitalized words. Use the self compiled abbreivations list\n",
    "'''\n",
    "def getLowerText(tokens, labels):\n",
    "\n",
    "    lowerLines = list(map(lambda x: any(i.isupper() and len(i) > 1\n",
    "                                      and i != 'RT' and i not in abbreviations for i in x), tokens))\n",
    "    \n",
    "    print(lowerLines)\n",
    "    \n",
    "    tokens = np.array(tokens)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    lowerLines = np.array(lowerLines)\n",
    "    \n",
    "    tokens = tokens[lowerLines]\n",
    "    labels = labels[lowerLines]\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "'''\n",
    "get either emoticon tweets or lowercase tweets depending\n",
    "on emojis argument\n",
    "'''\n",
    "def getModelsTwo(embeddingText, labels, emojis=None):\n",
    "\n",
    "    tokens = embeddingText.apply(lambda x: nltk.word_tokenize(x))\n",
    "    tokens = tokens.tolist()\n",
    "    \n",
    "    if emojis == None:\n",
    "        tokens, labels = getLowerText(tokens, labels) \n",
    "    else:\n",
    "        tokens, labels = getEmoticonText(tokens, emojis)\n",
    "     \n",
    "    text = pd.Series(tokens)\n",
    "    text = text.apply(lambda x: ' '.join(x))\n",
    "    labels = pd.Series(labels)\n",
    "\n",
    "    return text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, labels = getEmoticonText(tEmoji, labelsSchiz2, UNICODE_EMOJI)\n",
    "textEmoji = pd.Series(tokens)\n",
    "labels = pd.Series(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1EClean=['Lemma', 'Stopwords', 'Phrases', 'Emoticons']\n",
    "m2EClean=['Lemma', 'Stopwords', 'Phrases']\n",
    "cleanSchedule = [m1EClean, m2EClean, m1EClean, m2EClean]*2\n",
    "resultsPath = 'results/neural embeddings/emojiAll/'\n",
    "path1 = ['embeddings/Word2Vec/schiz/02 emoticons2/']\n",
    "path2 = ['embeddings/FastText/schiz/02 emoticons2/']\n",
    "resultsPath = 'results/neural embeddings/emoji/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSchizWordData2(textEmoji, labels, cleanSchedule, resultsPath, path1, path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoticon Models - emoticon only tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJGaUYyyP-fE"
   },
   "outputs": [],
   "source": [
    "#Looking at just emoticon tweets\n",
    "path1 = ['embeddings/Word2Vec/schiz/02 emoticons2/']\n",
    "path2 = ['embeddings/FastText/schiz/02 emoticons2/']\n",
    "resultsPath = 'results/neural embeddings/emoji/'\n",
    "cleanSchedule = [m1EClean, m2EClean] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Schizophrenia models with only tweets containing emoticons 1.Word 2.Character 3.Glove\n",
    "'''\n",
    "\n",
    "def getSchizEmojiWordData(cleanSchedule, resultsPath, path1, path2):\n",
    "\n",
    "    wordParametersLst = getParameters(wordParameters, emWordModels)\n",
    "    emWordModels = ['w2vemWCBSchiz', 'w2vemWCBEmSchiz', 'ftemWCBSchiz', 'ftemWCBEmSchiz']\n",
    "    emWordPaths = getFilePath(path1, path2, emWordModels)\n",
    "    wordParametersLst = getParameters(wordParameters, emWordPaths)\n",
    "    getSchizResults(emWordPaths, emWordModels, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, textSchiz2, labelsSchiz2)\n",
    "    \n",
    "def getSchizEmojiCharData(cleanSchedule, resultsPath, path1, path2):\n",
    "\n",
    "    charParametersLst = getParameters(charParameters, emWordModels)\n",
    "    emCharModels = ['w2vemCCBSchiz', 'w2vemCCBEMSchiz', 'ftemCCBSchiz', 'ftemCCBEMSchiz']\n",
    "    emCharPaths = getFilePath(path1, path2, emCharModels)\n",
    "    charParametersLst = getParameters(charParameters, emCharPaths)\n",
    "    getSchizResults(emCharPaths, emCharModels, charToken, cleanSchedule, \n",
    "                                        charParametersLst, textSchiz2, labelsSchiz2)\n",
    "\n",
    "def getSchizEmojiGlove(cleanSchedule, resultsPath, path1, path2):\n",
    "    \n",
    "    gloveModels = ['glove200']\n",
    "    cleanSchedule = [m2EClean]\n",
    "    gloveModels = [KeyedVectors.load_word2vec_format('embeddings/glove/glove200', binary=False)]\n",
    "    gloveParametersLst = getParameters(wordParameters, GloveModels)\n",
    "    getSchizResults(emCharPaths, gloveModels, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, textSchiz2, labelsSchiz2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSchizEmojiWordData(cleanSchedule, resultsPath, path1, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSchizEmojiCharData(cleanSchedule, resultsPath, path1, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSchizEmojiGlove(cleanSchedule, resultsPath, path1, path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1UXxmCuP-fP"
   },
   "source": [
    "### Capitalized Models - all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for capitalized models. Start with classification on all data using the capitalized embedding models\n",
    "then we look at just the subset of tweets which contains capitalized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = ['embeddings/Word2Vec/schiz/01 lowercase/all/']\n",
    "path2 = ['embeddings/FastText/schiz/01 lowercase/all/']\n",
    "cleanSchedule = [m2LClean, m1LClean, m2LClean, m1LClean]\n",
    "resultsPath = 'results/neural embeddings/lower/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Schizophrenia models, focus on capitalized words 1.Word 2.Character 3.Glove\n",
    "'''\n",
    "\n",
    "'''\n",
    "word models\n",
    "'''\n",
    "def getSchizLowerWordData(cleanSchedule, resultsPath, path1, path2):\n",
    "\n",
    "    lWordModels = ['w2vWCBlSchiz', 'w2vWCBSchiz', 'w2vWSGlSchiz', 'w2vWSGSchiz']\n",
    "    lWordPaths = utility.getFilePath(path1, path2, lWordModels)\n",
    "    wordParametersLst = utility.getParameters(wordParameters, lWordModels)\n",
    "    getResults(lWordPaths, lWordModels, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, textSchiz2, labelsSchiz2, resultsPath)\n",
    "\n",
    "'''\n",
    "char models\n",
    "'''\n",
    "def getSchizLowerCharData(cleanSchedule, resultsPath, path1, path2):\n",
    "\n",
    "    lCharModels = ['w2vWCBlSchiz', 'w2vWCBSchiz', 'w2vWSGlSchiz', 'w2vWSGSchiz']\n",
    "    lCharPaths = utility.getFilePath(path1, path2, lWordModels)\n",
    "    charParametersLst = utility.getParameters(charParameters, lCharModels)\n",
    "    getResults(lCharPaths, lCharModels, charToken, cleanSchedule, \n",
    "                                        charParametersLst, textSchiz2, labelsSchiz2, resultsPath)\n",
    "\n",
    "'''\n",
    "glove models\n",
    "'''\n",
    "def getSchizLowerGlove(cleanSchedule, resultsPath, path1, path2):\n",
    "    \n",
    "    gloveModels = ['glove200']\n",
    "    cleanSchedule = [m2EClean]\n",
    "    gloveModels = [KeyedVectors.load_word2vec_format('embeddings/glove/glove200', binary=False)]\n",
    "    gloveParametersLst = getParameters(wordParameters, GloveModels)\n",
    "    getSchizResults(emCharPaths, gloveModels, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, textSchiz2, labelsSchiz2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitalized Models - lowercase tweets only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImW1ZDb3P-fQ"
   },
   "outputs": [],
   "source": [
    "#Looking at just lowercase tweets\n",
    "lOnlyWordModels = ['w2vlWCBlSchiz', 'w2vlWCBSchiz', 'w2vlWSGlSchiz', 'w2vlWSGSchiz', 'ftlWCBlSchiz', 'ftlWCBSchiz', 'ftlWSGlSchiz', 'ftlWSGSchiz']\n",
    "lOnlyCharModes = ['']\n",
    "path1 = ['embeddings/Word2Vec/schiz/01 lowercase/lowerOnly/']\n",
    "path2 = ['embeddings/FastText/schiz/01 lowercase/lowerOnly/']\n",
    "lOnlyWordModels = utility.getFilePath(path1, path2, lWordModels)\n",
    "lOnlyCharModels  utility.getFilePath(path1, path2, lCharModels)\n",
    "cleanSchedule = [m2LClean, m1LClean, m2LClean, m1LClean]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WXCCf3EHP-fT"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Schizophrenia models with only tweets containing sequence of capitalized words 1.Word 2.Character 3.Glove\n",
    "'''\n",
    "def getSchizLowerData(cleanSchedule, resultsPath, path1, path2):\n",
    "\n",
    "    lOnlyWordModels = ['w2vlWCBlSchiz', 'w2vlWCBSchiz', 'w2vlWSGlSchiz', 'w2vlWSGSchiz', 'ftlWCBlSchiz', 'ftlWCBSchiz', 'ftlWSGlSchiz', 'ftlWSGSchiz']\n",
    "    \n",
    "    wordParametersLst = getParameters(wordParameters, lOnlyWordModels)\n",
    "    lWordModels = getFilePath(path1, path2, lWordModels)\n",
    "    wordParametersLst = getParameters(wordParameters, emWordPaths)\n",
    "    getSchizResults(emWordPaths, emWordModels, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, textSchiz2, labelsSchiz2)\n",
    "    \n",
    "def getSchizLowerData(cleanSchedule, resultsPath, path1, path2):\n",
    "\n",
    "    charParametersLst = getParameters(charParameters, emWordModels)\n",
    "    emCharPaths = getFilePath(path1, path2, emCharModels)\n",
    "    charParametersLst = getParameters(charParameters, emCharPaths)\n",
    "    getSchizResults(emCharPaths, emCharModels, charToken, cleanSchedule, \n",
    "                                        charParametersLst, textSchiz2, labelsSchiz2)\n",
    "\n",
    "def getSchizLowerGlove(cleanSchedule, resultsPath, path1, path2):\n",
    "    \n",
    "    gloveModels = ['glove200']\n",
    "    cleanSchedule = [m2EClean]\n",
    "    gloveModels = [KeyedVectors.load_word2vec_format('embeddings/glove/glove200', binary=False)]\n",
    "    gloveParametersLst = getParameters(wordParameters, GloveModels)\n",
    "    getSchizResults(emCharPaths, gloveModels, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, textSchiz2, labelsSchiz2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = ['embeddings/Word2Vec/schiz/01 lowercase/all/']\n",
    "path2 = ['embeddings/FastText/schiz/01 lowercase/all/']\n",
    "cleanSchedule = [m2LClean, m1LClean, m2LClean, m1LClean]\n",
    "resultsPath = 'results/neural embeddings/lower/'\n",
    "textSchiz = [[s] for s in textSchiz2.tolist()]\n",
    "lowerText = getLowerText(textSchiz2, labelsSchiz2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "word model\n",
    "'''\n",
    "def getSchizLowerData(cleanSchedule, resultsPath, path1, path2):\n",
    "\n",
    "    lOnlyWordModels = ['w2vlWCBlSchiz', 'w2vlWCBSchiz', 'w2vlWSGlSchiz', 'w2vlWSGSchiz', 'ftlWCBlSchiz', 'ftlWCBSchiz', 'ftlWSGlSchiz', 'ftlWSGSchiz']\n",
    "    \n",
    "    wordParametersLst = getParameters(wordParameters, lOnlyWordModels)\n",
    "    lWordModels = getFilePath(path1, path2, lWordModels)\n",
    "    wordParametersLst = getParameters(wordParameters, emWordPaths)\n",
    "    getSchizResults(emWordPaths, emWordModels, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, textSchiz2, labelsSchiz2)\n",
    "    \n",
    "'''\n",
    "lower model\n",
    "'''  \n",
    "def getSchizLowerGlove(cleanSchedule, resultsPath, path1, path2):\n",
    "    \n",
    "    lOnlyWordModels = ['w2vlWCBlSchiz', 'w2vlWCBSchiz', 'w2vlWSGlSchiz', 'w2vlWSGSchiz', 'ftlWCBlSchiz', 'ftlWCBSchiz', 'ftlWSGlSchiz', 'ftlWSGSchiz']\n",
    "\n",
    "    gloveModels = ['glove200']\n",
    "    cleanSchedule = [m2EClean]\n",
    "    gloveModels = [KeyedVectors.load_word2vec_format('embeddings/glove/glove200', binary=False)]\n",
    "    gloveParametersLst = getParameters(wordParameters, GloveModels)\n",
    "    getSchizResults(lOnlyWordModels, gloveModels, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, textSchiz2, labelsSchiz2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Datasets not used in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emCombWordModels = ['w2vWCBAll', 'w2vWCBEmAll', 'w2vWSGAll', 'w2vWSGEmAll', 'ftWCBAll', 'ftWCBEmAll', 'ftWSGAll', 'ftWSGEmAll']\n",
    "emCombCharModels = ['w2vCCBAll', 'w2vCCBEMAll', 'w2vCSGAll', 'w2vCSGEmAll', 'ftCCBAll', 'ftCCBEMAll', 'ftCSGAll', 'ftCSGEmAll']\n",
    "path1 = ['embeddings/Word2Vec/combined/']\n",
    "path2 = ['embeddings/FastText/combined/']\n",
    "emCombWordModels = getFilePath(path1, path2, emCombWordModels)\n",
    "emCombCharModels = getFilePath(path1, path2, emCombCharModels)\n",
    "cleanSchedule = [m1EClean, m2EClean, m1EClean, m2EClean] * 2\n",
    "charParametersLst = getParameters(charParameters, emCombCharModels)\n",
    "wordParametersLst = getParameters(wordParameters, emCombWordModels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenTypes = [False, False, False, False] * 2\n",
    "modelCombWords = getMLresult(methods, wordParametersLst, textSchiz2, labelsSchiz2, tokenTypes, cleanSchedule, emCombWordModels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenTypes = [True, True, True, True] * 2\n",
    "charCombWords = getMLresults(methods, charParametersLst, textSchiz2, labelsSchiz2, tokenTypes, cleanSchedule, emCombCharModels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9SXbyoQP-fe"
   },
   "source": [
    "### Combining embeddings with other our baseline set of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we combine the embedding models with a tf-idf matrix and the descroptive featuers such as sentiment and run them through the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHYXUo5rP-ff"
   },
   "outputs": [],
   "source": [
    "path1 = ['embeddings/Word2Vec/schiz/00 emoticons/all/']\n",
    "path2 = ['embeddings/FastText/schiz/00 emoticons/all/']\n",
    "cleanSchedule = [m1EClean, m2EClean, m1EClean, m2EClean] * 2\n",
    "resultsPath = 'results/neural embeddings/combined/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OjWNhFvP-fj"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "word model\n",
    "'''\n",
    "\n",
    "def getSchizCombWordData(cleanSchedule, resultsPath, path1, path2):\n",
    "\n",
    "    emCombWordModels = ['w2vWCBSchiz', 'w2vWCBEmSchiz', 'w2vWSGSchiz', 'w2vWSGEmSchiz', 'ftWCBSchiz', 'ftWCBEmSchiz', 'ftWSGSchiz', 'ftWSGEmSchiz']\n",
    "    emWordCombPaths = utility.getFilePath(path1, path2, emCombWordModels)\n",
    "    wordParametersLst = utility.getParameters(wordParameters, emCombWordModels)\n",
    "    getResults(emWordCombPaths, emCombWordModels, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, textSchiz2, labelsSchiz2, resultsPath, featureBoost=True)\n",
    " '''\n",
    "Character data\n",
    " '''   \n",
    "def getSchizCombCharData(cleanSchedule, resultsPath, path1, path2):\n",
    "\n",
    "    emCombCharModels = ['w2vCCBSchiz', 'w2vCCBEMSchiz', 'w2vCSGSchiz', 'w2vCSGEmSchiz', 'ftCCBSchiz', 'ftCCBEMSchiz', 'ftCSGSchiz', 'ftCSGEmSchiz']\n",
    "    emCombCharPaths = utility.getFilePath(path1, path2, emCombCharModels)\n",
    "    charParametersLst = utility.getParameters(charParameters, emCombCharModels)\n",
    "    getResults(emCombCharPaths, emCombCharModels, charToken, cleanSchedule, \n",
    "                                        charParametersLst, textSchiz2, labelsSchiz2, resultsPath, featureBoost=True)\n",
    "'''\n",
    "Glove data\n",
    "'''\n",
    "def getSchizCombGlove(cleanSchedule, resultsPath, path1, path2):\n",
    "    \n",
    "    gloveModels = ['glove200']\n",
    "    cleanSchedule = [m2EClean]\n",
    "    gloveModels = [KeyedVectors.load_word2vec_format('embeddings/glove/glove200', binary=False)]\n",
    "    gloveParametersLst = getParameters(wordParameters, GloveModels)\n",
    "    getSchizResults(emCharPaths, gloveModels, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, textSchiz2, labelsSchiz2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLXqrTsEP-fn",
    "outputId": "3ef4026f-a570-46cb-e5d3-db86cd8decc6"
   },
   "outputs": [],
   "source": [
    "getSchizCombWordData(cleanSchedule, resultsPath, path1, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSchizCombCharData(cleanSchedule, resultsPath, path1, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSchizCombGlove(cleanSchedule, resultsPath, path1, path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different size Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2EClean = ['Lemma', 'Stopwords', 'Phrases']\n",
    "cleanSchedule = [m2EClean]\n",
    "resultsPath = 'results/neural embeddings/size/ft/'\n",
    "x = pd.read_csv('data/dataIn/schiz/nonAnnFinalSchiz.csv')\n",
    "fileNum = len(x)\n",
    "modelW2v = 'w2vWSGEmSchiz'\n",
    "modelPathW2v = 'embeddings/size/w2v/w2vWSGEmSchiz'\n",
    "modelFt = 'ftWSGEmSchiz'\n",
    "modelPathFt = 'embeddings/size/ft/ftWSGEmSchiz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get the word classification results\n",
    "'''\n",
    "def getWordData(modelPaths, models, text, labels, cleanSchedule, resultsPath):\n",
    "\n",
    "    wordParametersLst = utility.getParameters(wordParameters, models)\n",
    "    getResults(modelPaths, models, wordToken, cleanSchedule, \n",
    "                                        wordParametersLst, text, labels, resultsPath)\n",
    "    \n",
    "def getSizeEmbeddings(textSet, modelPath, libraries):\n",
    "\n",
    "    for i in range(len(textSet)):\n",
    "        text = textSet[i]\n",
    "        text = pd.Series(text)\n",
    "        models = [w+str(i) for w in modelPath]\n",
    "        x, y = getWordModels(libraries, text, cleanSchedule, trainSchedule, models, emArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "loop through different sized w2v models\n",
    "'''\n",
    "\n",
    "for i in range(fileNum):\n",
    "    modelPaths = [modelPathW2v+str(i)]\n",
    "    models = [modelW2v+str(i)]\n",
    "    getWordData(modelPaths, models, textSchiz2, labelsSchiz2, cleanSchedule, resultsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "loop through for ft different sized models\n",
    "'''\n",
    "\n",
    "for i in range(fileNum):\n",
    "    modelPaths = [modelPathFt+str(i)]\n",
    "    models = [modelFt+str(i)]\n",
    "    getWordData(modelPaths, models, textEmoji, labelsSchiz2, cleanSchedule, resultsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Embeddings - This is not used in the final paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackEmbeddings():\n",
    "    embeddingMatrix = np.concatenate((embeddingsSet), axis=1) \n",
    "    return embeddingMatrix\n",
    "\n",
    "def getPCA(embedding):\n",
    "    pca = PCA(n_components=200)\n",
    "    pComponents = pca.fit_transform(embedding)\n",
    "    return pComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "testw2v = Word2Vec.load('embeddings/Word2Vec/schiz/00 emoticons/all/w2vCCBSchiz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFt = Word2Vec.load('embeddings/FastText/schiz/00 emoticons/all/ftCCBSchiz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "testGlove = KeyedVectors.load_word2vec_format('embeddings/glove/glove200', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens\n",
      "Lemma\n",
      "Stopwords\n",
      "Phrases\n",
      "Emoticons\n"
     ]
    }
   ],
   "source": [
    "tokensSchiz1 = initializePreProcessing(textSchiz1, False, cleanMethods=['Tokens', 'Lemma', 'Stopwords', 'Phrases', 'Emoticons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gregoryverghese/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/gregoryverghese/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "models = [testFt, testGlove, testw2v]\n",
    "embeddings = [getEmbeddingData(m, tokensSchiz1) for m in models]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "04 Machine Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
