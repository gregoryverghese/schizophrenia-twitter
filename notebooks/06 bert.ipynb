{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains code for pytorch bert implementation. The code was adapted from \n",
    "https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0823 10:45:37.154357 140735584027520 tokenization.py:161] The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "100%|██████████| 213450/213450 [00:00<00:00, 828928.42B/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNameSchiz1 = 'data/dataOut/schiz/annFinalSchiz_1.csv'\n",
    "fileNameSchiz2 = 'data/dataOut/schiz/annFinalSchiz_2.csv'\n",
    "fileNameStig = 'data/dataOut/stigma/annFinalStig.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "socialDf = pd.read_csv(fileNameSchiz2, encoding='utf-8')\n",
    "textSchiz2 = socialDf['Tweet']\n",
    "labelsSchiz2 = socialDf['Classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "socialDf = pd.read_csv(fileNameSchiz1, encoding='utf-8')\n",
    "tweetsSchiz1 = socialDf.Tweet.values\n",
    "labelsSchiz1 = socialDf.Classification.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firs we prepare the text in the Bert format with the [CLS] separator and then convert into tensor form. Once we have the data in the right form, including the attention masks we train the model computing a foward pass then using the BertAdam optimizer to minimize our loss. Once this is done we put the model into evaluation mode and evaluate. Can take a little while to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBertFormat(tweets):\n",
    "    tweets = ['[CLS] ' + s + ' [SEP]' for s in tweets]\n",
    "    tokens = [tokenizer.tokenize(t) for t in tweets]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTorchTensors(**kwargs):\n",
    "    return {k:torch.tensor(v) for k,v in kwargs.items()}\n",
    "\n",
    "\n",
    "def getParameters(model):\n",
    "    \n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    \n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizerParameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}]\n",
    "    \n",
    "    return optimizerParameters\n",
    "        \n",
    "        \n",
    "def trainModel(model, trainDataLoader, optimizer, epochs=4):\n",
    "        \n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "        for step, batch in enumerate(trainDataLoader):\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)   \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "            print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "        \n",
    "        \n",
    "def evalModel(model, optimizer, valDataLoader):\n",
    "        \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in valDataLoader:\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "   \n",
    "           \n",
    "\n",
    "def getBert(text, device, batch=16):\n",
    "    \n",
    "    tokens = getBertFormat(text)\n",
    "    #maxLength = max([len(t) for t in tokens]) + 2\n",
    "    maxLength = 200\n",
    "    ids = [tokenizer.convert_tokens_to_ids(t) for t in tokens]\n",
    "    inputIds = pad_sequences(ids, maxlen=maxLength, dtype='long', truncating='post', padding='post')\n",
    "    \n",
    "    attentionMasks = [list(map(lambda x: float(x>0), s)) for s in inputIds]\n",
    "    \n",
    "    trainInputs, valInputs, trainLabels, valLabels = train_test_split(inputIds, labelsSchiz1, random_state=42, test_size=0.2)\n",
    "    trainMasks, valMasks, _, _ = train_test_split(attentionMasks, inputIds, random_state=42, test_size=0.2)\n",
    "    \n",
    "    inputs = getTorchTensors(trainInputs=trainInputs, valInputs=valInputs, trainLabels=trainLabels, valLabels=valLabels)\n",
    "    masks = getTorchTensors(trainMasks=trainMasks, valMasks=valMasks)\n",
    "    \n",
    "    train_data = TensorDataset(inputs['trainInputs'], masks['trainMasks'], inputs['trainLabels'])\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch)\n",
    "\n",
    "    valData = TensorDataset(inputs['valInputs'], masks['valMasks'], inputs['valLabels'])\n",
    "    valSampler = SequentialSampler(valData)\n",
    "    valDataLoader = DataLoader(valData, sampler=valSampler, batch_size=batch)\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-large-cased\", num_labels=2)\n",
    "    optimizerParameters = getParameters(model)\n",
    "    optimizer = BertAdam(optimizerParameters, lr=2e-5, warmup=.1)\n",
    "        \n",
    "    model = trainModel(model, valDataLoader, optimizer)\n",
    "    model = evalModel(model, valDataLoader, optimizer)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1242874899/1242874899 [08:37<00:00, 2402193.65B/s]\n",
      "W0823 10:54:40.950658 140735584027520 optimization.py:46] t_total value of -1 results in schedule not being applied\n",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8509582281112671\n",
      "Train loss: 0.7697545289993286\n",
      "Train loss: 0.7695855498313904\n",
      "Train loss: 0.7783420830965042\n",
      "Train loss: 0.7576043844223023\n",
      "Train loss: 0.7294232845306396\n",
      "Train loss: 0.7202801874705723\n",
      "Train loss: 0.7079262137413025\n",
      "Train loss: 0.781946619351705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|██▌       | 1/4 [15:58<47:55, 958.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8015725314617157\n",
      "Train loss: 0.5950214862823486\n",
      "Train loss: 0.6569541692733765\n",
      "Train loss: 0.6484621365865072\n",
      "Train loss: 0.6931104511022568\n",
      "Train loss: 0.6918761372566223\n",
      "Train loss: 0.7162438333034515\n",
      "Train loss: 0.7241217408861432\n",
      "Train loss: 0.7201010212302208\n",
      "Train loss: 0.7144479751586914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 2/4 [31:56<31:56, 958.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.7020955622196198\n",
      "Train loss: 0.631625235080719\n",
      "Train loss: 0.6417377591133118\n",
      "Train loss: 0.6175366640090942\n",
      "Train loss: 0.5411438345909119\n",
      "Train loss: 0.6146926403045654\n",
      "Train loss: 0.6852958798408508\n",
      "Train loss: 0.7031082596097674\n",
      "Train loss: 0.676782812923193\n",
      "Train loss: 0.6537459426456027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  75%|███████▌  | 3/4 [47:54<15:58, 958.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6243840396404267\n",
      "Train loss: 0.42754799127578735\n",
      "Train loss: 0.5440468192100525\n",
      "Train loss: 0.5130421916643778\n",
      "Train loss: 0.5658839643001556\n",
      "Train loss: 0.6946876287460327\n",
      "Train loss: 0.6330302357673645\n",
      "Train loss: 0.6203817980630058\n",
      "Train loss: 0.5989611111581326\n",
      "Train loss: 0.635890758699841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 100%|██████████| 4/4 [1:04:09<00:00, 963.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5896166175603866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3ea7e7764e38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetBert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweetsSchiz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-4beae6252254>\u001b[0m in \u001b[0;36mgetBert\u001b[0;34m(text, device, batch)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevalModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-4beae6252254>\u001b[0m in \u001b[0;36mevalModel\u001b[0;34m(model, optimizer, valDataLoader)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevalModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mnb_eval_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_eval_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "getBert(tweetsSchiz1, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
