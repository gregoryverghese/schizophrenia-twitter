{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notebook contains all code to generate embedding models for mental health related twitter data. The genisim package \n",
    "is used along with Word2Vec and FastText to set up the embeddings. Dataset is cleaned according to each models\n",
    "specification. The notebook leverages functions in the preprocessing and utility script which can be found in the scripts folder. For more details on each model please refer to details in the corresponding papers Appendix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4W8QpMU1GjYb"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility\n",
    "import preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ft2J4XUYKf_s",
    "outputId": "1c05aa2a-7ae0-4b5f-9635-656acf598857"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from emoji import UNICODE_EMOJI\n",
    "import multiprocessing\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim as g\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ZjtVy7liKf_w",
    "outputId": "98f50169-67fc-428a-b98b-e06f656082df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gregoryverghese/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gregoryverghese/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gregoryverghese/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cores = multiprocessing.cpu_count()\n",
    "NOOCC_INDEX = 0\n",
    "NOOCC_TOKEN = 'NOOCC'\n",
    "MODEL_NUM = 4\n",
    "abbreviations = pd.read_csv('data/other/abbreviations.csv')['Abbreviation'].tolist()\n",
    "abbreviations = [str(a).strip() for a in abbreviations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = [2, 3, 4, 5]\n",
    "window = [2, 3 , 4, 5]\n",
    "size = [25, 50, 100, 200]\n",
    "sample=6e-5\n",
    "alpha=0.05\n",
    "min_alpha=0.0007 \n",
    "negative=20\n",
    "sgTrain = 1\n",
    "cbowTrain = 0\n",
    "m1Clean=['Tokens', 'Lemma', 'Stopwords', 'Phrases', 'Emoticons']\n",
    "m2Clean=['Tokens', 'Lemma', 'Stopwords', 'Phrases']\n",
    "m1LClean=['Tokens', 'Lemma', 'Stopwords', 'Phrases', 'Lowercase']\n",
    "m2LClean=['Tokens', 'Lemma', 'Stopwords', 'Phrases']\n",
    "emArgs = [min_count[0], window[0], size[3], sample, alpha, min_alpha, negative]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e-2ilImDKf_0"
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FL-Hd81CKf_3"
   },
   "outputs": [],
   "source": [
    "fileSchiz = 'data/dataIn/schiz/nonAnnFinalSchiz.csv'\n",
    "textSchiz = pp.getFile(fileSchiz, 'Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M4j2RyTQFl0F"
   },
   "outputs": [],
   "source": [
    "fileStigma = 'data/dataIn/stigma/nonAnnFinalStig.csv'\n",
    "textStigma = pp.getFile(fileStigma, 'Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yMD5fnFGGjYy"
   },
   "outputs": [],
   "source": [
    "textAll = pd.concat((textSchiz, textStigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0uiCC42KgAB"
   },
   "source": [
    "## Word2Vec and FastText Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6WQJh8B5KgAB"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "embedding class contains code to train and save down a embedding model\n",
    "using Gensim Library. Code is compatible with either Word2Vec or FastText\n",
    "libraries.\n",
    "'''\n",
    "class EmbeddingModel():\n",
    "    def __init__(self, library, tokens, name):\n",
    "        self.lib = library\n",
    "        self.tokens = tokens\n",
    "        self.name = name\n",
    "        self.model = None\n",
    "\n",
    "    '''\n",
    "    train embedding models using arguments in emArgs\n",
    "    '''\n",
    "    def getEmbeddings(self, trainMethod, emArgs):\n",
    "        \n",
    "        print(self.lib)\n",
    "        \n",
    "        model = self.lib(min_count=emArgs[0], window=emArgs[1], size=emArgs[2], sample=emArgs[3], alpha=emArgs[4], min_alpha=emArgs[5], negative=emArgs[6], workers=20, sg=trainMethod)\n",
    "        model.build_vocab(self.tokens, progress_per=10000)\n",
    "        model.train(self.tokens, total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
    "        model.init_sims(replace=True)\n",
    "        self.save(model)\n",
    "        self.model = model\n",
    "    \n",
    "    '''\n",
    "    save model down\n",
    "    '''\n",
    "    def save(self, model):\n",
    "        model.save(self.name)\n",
    "    \n",
    "    '''\n",
    "    return similar words\n",
    "    '''\n",
    "    def getSimilar(self, model, word):\n",
    "        return self.model.wv.most_similar(positive=[word])\n",
    "    \n",
    "    '''\n",
    "    get the word index using the vocabulary and add an OOV token (NOOCC_TOKEN)\n",
    "    for word out of vocabulary\n",
    "    '''\n",
    "    def getWordIndex(self, newWord=NOOCC_TOKEN, newIndex=NOOCC_INDEX ):\n",
    "        \n",
    "        wordIndex = {k: v.index for k, v in self.model.wv.vocab.items()}\n",
    "        self.model.wv.vectors = np.insert(self.model.wv.vectors, [newIndex], self.model.wv.vectors.mean(0), axis=0)\n",
    "        wordIndex = {word:(index+1) if index>= newIndex else index for word, index in wordIndex.items()}\n",
    "        wordIndex[newWord] = newIndex\n",
    "\n",
    "        return wordIndex\n",
    "\n",
    "    '''\n",
    "    get train data using wordIndex\n",
    "    '''\n",
    "    def getIndexData(self, xText, labels, wordIndex):\n",
    "        xTrain = [[wordIndex[tok] if tok in wordIndex else wordIndex[NOOCC_TOKEN] for tok in s] for s in xText]\n",
    "        return (np.array(xTrain), np.array(labels))\n",
    "\n",
    "    '''\n",
    "    count number of missing words from model\n",
    "    '''\n",
    "    def countMissing(self, text, wordIndex):\n",
    "        return sum([1 for s in text for tok in s if tok not in wordIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQbvAjHmKgAF"
   },
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LWQap41dKgAK"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "call preprocessing class and set up cleaning object\n",
    "'''\n",
    "def initializePreProcessing(text, tokenType, cleanMethods=['Tokens', 'Lemma', 'Stopwords', 'Phrases', 'Emoticons']):\n",
    "\n",
    "    social = pp.SocialPreProcessing(text, tokenType)\n",
    "    socialClean = social.clean(cleanMethods)\n",
    "    return socialClean\n",
    "\n",
    "'''\n",
    "call embedding class and set up embedding object\n",
    "'''\n",
    "def initializeEmbedding(library, tokens, trainMethod, name, emArg):\n",
    "    \n",
    "    emModel = EmbeddingModel(library, tokens, name)\n",
    "    emModel.getEmbeddings(trainMethod, emArg)\n",
    "  \n",
    "    return emModel\n",
    "\n",
    "\n",
    "'''\n",
    "train all embedding models and get their word vocabulary\n",
    "'''\n",
    "def getEmbeddings(libraries, embeddingText, tokenTypes, cleanSchedule, trainSchedule, names, modelNum, emArgs):\n",
    "  \n",
    "    \n",
    "    embeddingTexts = map(initializePreProcessing, [embeddingText]*modelNum, tokenTypes, cleanSchedule)\n",
    "    emModels = map(initializeEmbedding, libraries, embeddingTexts, trainSchedule, names, emArgs)\n",
    "    indicies = map(lambda x: x.getWordIndex(), emModels)\n",
    "    return indicies, emModels\n",
    "\n",
    "\n",
    "'''\n",
    "set up word models to get word embeddings\n",
    "'''\n",
    "def getWordModels(libraries, embeddingText, cleanSchedule, trainSchedule, wordNames, emArgs):\n",
    "    modelNum = len(cleanSchedule)\n",
    "    tokenTypes = [False]*modelNum\n",
    "    emArgs = modelNum*[emArgs]\n",
    "    wordIndicies, wordModels = getEmbeddings(libraries, embeddingText, tokenTypes, cleanSchedule, trainSchedule, wordNames, modelNum, emArgs)\n",
    "    return wordIndicies, modelNum\n",
    "    \n",
    "'''\n",
    "set up char models to get char embeddings\n",
    "'''\n",
    "def getCharModels(libraries, embeddingText, cleanSchedule, trainSchedule, charNames, emArgs):\n",
    "    modelNum = len(cleanSchedule)\n",
    "    tokenTypes = [True]*modelNum\n",
    "    emArgs = modelNum*[emArgs]\n",
    "    charIndicies, charModels = getEmbeddings(libraries, embeddingText, tokenTypes, cleanSchedule, trainSchedule, charNames, modelNum, emArgs)\n",
    "    return charIndicies, charModels\n",
    "\n",
    "\n",
    "'''\n",
    "call code to get models focused on emoticons. Return both character and word models\n",
    "'''\n",
    "def initializeEmoticonModels(libraries, embeddingText, cleanSchedule, trainSchedule, args, wNames, cNames):\n",
    "  \n",
    "    wordIndicies, wordModels = getWordModels(libraries, embeddingText, cleanSchedule, trainSchedule, wNames, args)\n",
    "    charIndicies, charModels = getCharModels(libraries, embeddingText, cleanSchedule, trainSchedule, cNames, args)\n",
    "  \n",
    "    return (wordModels, charModels)\n",
    "\n",
    "\n",
    "'''\n",
    "call code to get models focused on capitalized words. Return both character and word models\n",
    "'''\n",
    "def initializeLowerModels(libraries, embeddingText, cleanSchedule, trainSchedule, emArgs, wNames, cNames):\n",
    "  \n",
    "    wordIndicies, wordModels = getWordModels(libraries, embeddingText, cleanSchedule, trainSchedule, wNames, emArgs)\n",
    "    charIndicies, charModels = getCharModels(libraries, embeddingText, cleanSchedule, trainSchedule, cNames, emArgs)\n",
    "  \n",
    "    return (wordModels, charModels)\n",
    "\n",
    "\n",
    "'''\n",
    "return tweets with emoticons\n",
    "'''\n",
    "def getEmoticonText(tokens, emojis):\n",
    "\n",
    "    emojiLines = list(map(lambda x: any(i in emojis for i in x), tokens))\n",
    "    tokens = np.array(tokens)\n",
    "    emojiLines = np.array(emojiLines)\n",
    "  \n",
    "    tokens = tokens[emojiLines]\n",
    " \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaG3tFPeK4KF"
   },
   "source": [
    "## Mental health Stigma Dataset - Not in project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wozZ6YP6GjZQ"
   },
   "outputs": [],
   "source": [
    "trainSchedule = [cbowTrain, sgTrain,] * 2\n",
    "cleanSchedule = [m2Clean, m2Clean] *2\n",
    "libraries = [Word2Vec]* 2 + [FastText] * 2\n",
    "args = [min_count[0], window[0], size[3], sample, alpha, min_alpha, negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c__k--0pKgAO"
   },
   "outputs": [],
   "source": [
    "wordNames = ['w2vWCBStig', 'w2vWCBEmStig', 'ftWSGStig', 'ftWSGEmStig']\n",
    "charNames = ['w2vCCBStig', 'w2vCCBEMStig', 'ftCSGStig', 'ftCSGEmStig']\n",
    "paths = ['embeddings/Word2Vec/stigma/']*2 + ['embeddings/FastText/stigma/']*2 \n",
    "wordNames = getFilePath(paths, wordNames)\n",
    "charNames = getFilePath(paths, charNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "MGL00kusGjZb",
    "outputId": "5a6a75fe-2387-41be-e008-72956f9f043a"
   },
   "outputs": [],
   "source": [
    "modelsStigma = initializeEmoticonModels(libraries, textStigma, cleanSchedule, trainSchedule, args, wordNames, charNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VEgyLRCzGjZh"
   },
   "source": [
    "## Schizophrenia Stigma Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concentrate on embedding models with emoticons and without emoticons. Model code below can be found in the appendix\n",
    "of the paper. The pipelines below take the data and perform all the NLP preprocessing according to the model being\n",
    "constructed. For example the model 'w2vWCBSchiz' doesn't have the letters Em in it and therfore it will use m1Clean to remove the emoticons. Train schedule also determines if Skip Gram or CBOW is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHsiEVJ8GjZl"
   },
   "source": [
    "### Emoticons This first set of models looks at all data and remove emoticons accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eX6l4AWWGjZm"
   },
   "outputs": [],
   "source": [
    "trainSchedule = [cbowTrain, cbowTrain, sgTrain, sgTrain] * 2\n",
    "cleanSchedule = [m1Clean, m2Clean, m1Clean, m2Clean]* 2\n",
    "libraries = [Word2Vec]*4 + [FastText] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DSZiOrqPGjZp",
    "outputId": "1bdadaa4-5fbe-432d-f15c-fa6a070e54a0"
   },
   "outputs": [],
   "source": [
    "wordNames = ['w2vWCBSchiz', 'w2vWCBEmSchiz', 'w2vWSGSchiz', 'w2vWSGEmSchiz', 'ftWCBSchiz', 'ftWCBEmSchiz', 'ftWSGSchiz', 'ftWSGEmSchiz']\n",
    "charNames = ['w2vCCBSchiz', 'w2vCCBEMSchiz', 'w2vCSGSchiz', 'w2vCSGEmSchiz', 'ftCCBSchiz', 'ftCCBEMSchiz', 'ftCSGSchiz', 'ftCSGEmSchiz']\n",
    "path1 = ['embeddings/Word2Vec/schiz/']\n",
    "path2 = ['embeddings/FastText/schiz/']\n",
    "wordNames = utility.getFilePath(path1, path2, wordNames)\n",
    "charNames = utility.getFilePath(path1, path2, charNames)\n",
    "libraries = [Word2Vec]*4 + [FastText]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsSchiz = initializeEmoticonModels(libraries, textSchiz, cleanSchedule, trainSchedule, emArgs, wordNames, charNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lr3VujcBGjZ1",
    "outputId": "9b4ee41f-37de-47ca-83ef-1ae4d3674541"
   },
   "outputs": [],
   "source": [
    "#emWordNames = ['w2vemWCBSchiz', 'w2vemWCBEmSchiz', 'w2vemWSGSchiz', 'w2vemWSGEmSchiz', 'ftemWCBSchiz', 'ftemWCBEmSchiz', 'ftemWSGSchiz', 'ftemWSGEmSchiz']\n",
    "#emCharNames = ['w2vemCCBSchizSchiz', 'w2vemCCBEMSchiz', 'w2vemCSGSchiz', 'w2vemCSGEmSchiz', 'ftemCCBSchizSchiz', 'ftemCCBEMSchiz', 'ftemCSGSchiz', 'ftemCSGEmSchiz']\n",
    "#paths = ['embeddings/Word2Vec/schiz/emoticons/']*4 + ['embeddings/FastText/schiz/emoticons/']*4\n",
    "#emWordNames = getFilePath(paths, emWordNames)\n",
    "#emWharNames = getFilePath(paths, emCharNames)\n",
    "#emojiTexts = getModelsTwo(textSchiz, UNICODE_EMOJI)\n",
    "#modelsStigma = initializeEmoticonModels(libraries, textSchiz, cleanSchedule, trainSchedule, emArgs, emWordNames, emCharNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoticon work around for NLTK problem. Load the data saved down by Spacey pre-Tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due NLTK tokenizer not tokenizing all emoticons, I use SpaceyMoji. Unfortunately this is a python 3 package so it is in a separate notebook, 08 SpaceyMoji, and I save down the tokenized data. Then I load in the SpaceyMoji tokenized data and then create all the relevant embedding models here. Create embedding models for both word and character tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileEmoji = 'data/dataOut/schiz/emoji/nonAnnFinalSchizEmoji.csv'\n",
    "textEmoji = pp.getFile(fileEmoji, 'Tweet')\n",
    "textEmoji = textEmoji.apply(lambda x: x[1:len(x)-1])\n",
    "textEmoji = textEmoji.apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileEmojiChar = 'data/dataOut/schiz/emoji/nonAnnFinalSchizEmojiChar.csv'\n",
    "textEmojiChar = pp.getFile(fileEmojiChar, 'Tweet')\n",
    "textEmojiChar = textEmojiChar.apply(lambda x: x[1:len(x)-1])\n",
    "textEmojichar = textEmojiChar.apply(lambda x: x.split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emWordNames = ['w2vemWCBSchiz', 'w2vemWCBEmSchiz', 'w2vemWSGSchiz', 'w2vemWSGEmSchiz', 'ftemWCBSchiz', 'ftemWCBEmSchiz', 'ftemWSGSchiz', 'ftemWSGEmSchiz']\n",
    "emCharNames = ['w2vemCCBSchizSchiz', 'w2vemCCBEMSchiz', 'w2vemCSGSchiz', 'w2vemCSGEmSchiz', 'ftemCCBSchizSchiz', 'ftemCCBEMSchiz', 'ftemCSGSchiz', 'ftemCSGEmSchiz']\n",
    "path1 = ['embeddings/Word2Vec/schiz/02 emoticons2/']\n",
    "path2 = ['embeddings/FastText/schiz/02 emoticons2/']\n",
    "m1Clean=['Lemma', 'Stopwords', 'Phrases', 'Emoticons']\n",
    "m2Clean=['Lemma', 'Stopwords', 'Phrases']\n",
    "cleanSchedule = [m1Clean, m2Clean, m1Clean, m2Clean]* 2\n",
    "trainSchedule = [cbowTrain, cbowTrain, sgTrain, sgTrain] * 2\n",
    "libraries = [Word2Vec]*4 + [FastText]*4\n",
    "emWordNames = utility.getFilePath(path1, path2, emWordNames)\n",
    "emCharNames = utility.getFilePath(path1, path2, emCharNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD TOKENS\n",
    "wordIndicies, wordModels = getWordModels(libraries, textEmoji, cleanSchedule, trainSchedule, emWordNames, emArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHARACTER TOKENS\n",
    "charIndicies, charModels = getCharModels(libraries, textEmojiChar, cleanSchedule, trainSchedule, emCharNames, emArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGAcJ8YHGjZ5"
   },
   "source": [
    "### Capitalized Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This follows the same logic as above except we add in the lowercase cleaning function if we are looking at models that should all be lowercase. Same pipeline is followed, NLP prepocessing and then we train the embeddings. These are saved down in the lowercase folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xiNg1vd8GjaB"
   },
   "outputs": [],
   "source": [
    "lCharNames = ['w2vCCBSchiz', 'w2vCCBlSchiz', 'w2vCSGSchiz', 'w2vCSGlSchiz''ftCCBSchiz', 'ftCCBlSchiz', 'ftCSGSchiz', 'ftCSGlSchiz']\n",
    "lWordNames = ['w2vWCBSchiz','w2vWCBlSchiz', 'w2vWSGSchiz', 'w2vWSGlSchiz', 'ftWCBSchiz','ftWCBlSchiz', 'ftWSGSchiz', 'ftWSGlSchiz']\n",
    "path1 = ['embeddings/Word2Vec/schiz/01 lowercase/all/']\n",
    "path2 = ['embeddings/FastText/schiz/01 lowercase/all/']\n",
    "lWordNames = utility.getFilePath(path1, path2, lWordNames)\n",
    "lCharNames = utility.getFilePath(path1, path2, lCharNames)\n",
    "libraries = [Word2Vec]*4 + [FastText]*4\n",
    "trainSchedule = [cbowTrain, cbowTrain, sgTrain, sgTrain] * 2\n",
    "cleanSchedule = [m1LClean, m2LClean, m1LClean, m2LClean] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = initializeLowerModels(libraries, textSchiz, cleanSchedule, trainSchedule, emArgs, lWordNames, lCharNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we train embedding models with only tweets that contain lowercase words. Given this led to around a small number of tweets ca.3000, it was decided to not include this in the paper because the size of the dataset became the overwhelming factor in the classification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m522MCqEGjaI"
   },
   "outputs": [],
   "source": [
    "lCharNames = ['w2vlCCBSchiz', 'w2vlCCBlSchiz', 'w2vlCSGSchiz', 'w2vlCSGlSchiz', 'ftlCCBSchiz', 'ftlCCBlSchiz', 'ftlCSGSchiz', 'ftlCSGlSchiz']\n",
    "lWordNames = ['w2vlWCBSchiz', 'w2vlWCBlSchiz', 'w2vlWSGSchiz', 'w2vlWSGlSchiz', 'ftlWCBSchiz', 'ftlWCBlSchiz', 'ftlWSGSchiz', 'ftlWSGlSchiz']\n",
    "lowerTexts = getModelsTwo(textSchiz)\n",
    "models = initializeLowerModels(libraries, textSchiz, cleanSchedule, trainSchedule, emArgs, lWordNames, lCharNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ED-tad-lGjaL"
   },
   "source": [
    "## Combined schizophrenia and stigma - Not used in the final paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined the two datasets for the embedding models to see if this increased the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZsBVlmjGjaM"
   },
   "outputs": [],
   "source": [
    "emWordNames = ['w2vWCBAll', 'w2vWCBEmAll', 'w2vWSGAll', 'w2vWSGEmAll', 'ftWCBAll', 'ftWCBEmAll', 'ftWSGAll', 'ftWSGEmAll']\n",
    "emCharNames = ['w2vCCBAll', 'w2vCCBEMAll', 'w2vCSGAll', 'w2vCSGEmAll', 'ftCCBAll', 'ftCCBEMAll', 'ftCSGAll', 'ftCSGEmAll']\n",
    "trainSchedule = [cbowTrain, cbowTrain, sgTrain, sgTrain] * 2\n",
    "cleanSchedule = [m2Clean, m2Clean, m2Clean, m2Clean] * 2\n",
    "modelsStigma = initializeEmoticonModels(libraries, textAll, cleanSchedule, trainSchedule, emArgs, emWordNames, emCharNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_cQQtyzubAwy"
   },
   "outputs": [],
   "source": [
    "#Get training data\n",
    "trainText = map(initializePreProcessing, [text])\n",
    "getIndexData = map(lambda f, x, y, z: f.getIndexData(x, y, z), emModels, trainText, [labels], wordIndicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3XW-eD2mKgAp"
   },
   "outputs": [],
   "source": [
    "#Check number of missing words\n",
    "total = sum([1 for s in trainText[0] for tok in s])\n",
    "missing = map(lambda f, x, y: f.countMissing(x, y), emModels, trainText, wordIndicies)\n",
    "missing = map(lambda x, y: x/float(y), missing, [total]*len(missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was used to test how size affects the embedding models performance. This is in the final paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "loops through the data and incrementally and splits it up in multiples of 100\n",
    "'''\n",
    "def getSizeEmbeddings(textSet, modelPath, libraries):\n",
    "\n",
    "    for i in range(len(textSet)):\n",
    "        text = textSet[i]\n",
    "        text = pd.Series(text)\n",
    "        models = [w+str(i) for w in modelPath]\n",
    "        x, y = getWordModels(libraries, text, cleanSchedule, trainSchedule, models, emArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "textSchizLst = textSchiz.tolist()\n",
    "schizSets = [textSchizLst[0:i+100] for i in range(0, len(textSchizLst), 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2Clean=['Tokens', 'Lemma', 'Stopwords', 'Phrases']\n",
    "trainSchedule = [sgTrain]\n",
    "cleanSchedule = [m2Clean]\n",
    "libraries = [Word2Vec]\n",
    "w2vWordPath = ['embeddings/size/w2v/w2vWSGEmSchiz']\n",
    "ftWordNames = ['embeddings/size/ft/ftWSGEmSchiz']\n",
    "librariesFt = [FastText]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSizeEmbeddings(schizSets, ftWordNames, librariesFt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rEKoKl7-KgAx"
   },
   "source": [
    "### ELMO - This is not in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J5qwYGCv9g_t"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vH_f9x5r9g_y",
    "outputId": "8893ea5a-a5b2-415f-f551-495f5cc91950"
   },
   "outputs": [],
   "source": [
    "trainList = embeddingStigma.tolist()\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Dga1i1Q9g_2"
   },
   "outputs": [],
   "source": [
    "def getELMO(elmo, text):\n",
    "    \n",
    "    embeddings = elmo(text, signature='default', as_dict=True)['elmo']\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    embedding = sess.run(tf.reduce_mean(embeddings,1))\n",
    "    sess.close()\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a57LAZpD9g_3"
   },
   "outputs": [],
   "source": [
    "xTrain = [[sent] for sent in trainList]\n",
    "elmoEmbed = [getELMO(elmo, sent) for sent in xTrain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VngMtKgcYIGf"
   },
   "outputs": [],
   "source": [
    "elmoEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAn_Km9r9g_5"
   },
   "outputs": [],
   "source": [
    "pickle_out = open(\"elmo_train_03032019.pickle\",\"wb\")\n",
    "pickle.dump(elmoEmbed, pickle_out)\n",
    "pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "03 Embeddings.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
